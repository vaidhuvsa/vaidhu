# -*- coding: utf-8 -*-
"""Copy of Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FFg3_cGNgd21wUFXGvJKPLIR_DZ0CKvd

Upload the Dataset
"""

from google.colab import files
uploaded = files.upload()

"""Load the Dataset"""

from google.colab import files
import pandas as pd
import io

# Upload the file and capture its content, but only if 'uploaded' is not already defined
if 'uploaded' not in locals():
    uploaded = files.upload()
else:
    print("File already uploaded. Skipping upload.")

# Get the filename from the uploaded dictionary
filename = list(uploaded.keys())[0]

# Read the file using io.BytesIO to handle uploaded content
df = pd.read_csv(io.BytesIO(uploaded[filename]))

# Display the first few rows
df.head()

""" Data Exploration"""

# Check the basic info about the dataset
df.info()

# Summary statistics
df.describe()

"""Check for Missing Values and Duplicates"""

# Check for missing values
df.isnull().sum()

# Check for duplicates
df.duplicated().sum()

""". Visualize a Few Features"""

import matplotlib.pyplot as plt
import seaborn as sns

# Replace 'Actual_Feature_Name' with the actual name of the column in your dataframe
# For example, if your column is named 'Price', change the line below to:
# actual_feature_name = 'Price'
actual_feature_name = df.columns[0]  # Using the first column as an example

# Visualize distribution of a feature
sns.histplot(df[actual_feature_name])

# Visualize relationships between features
sns.pairplot(df)
plt.show()

""" Identify Target and Features"""

# Assuming 'Accident_Severity' is the target, or a similar named column is the target
target_column = 'Accident_Severity'  # Replace with actual column name if different

# Check if the target column exists in the DataFrame
if target_column in df.columns:
    X = df.drop(columns=[target_column])
    y = df[target_column]
else:
    print(f"Error: Column '{target_column}' not found in the DataFrame.")
    # Handle the error, e.g., by raising an exception or skipping the operation
    # ...

"""Convert Categorical Columns to Numerical"""

# Convert categorical columns to numerical (label encoding)
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

# Apply label encoding to each categorical column
for col in df.select_dtypes(include=['object']).columns:
    df[col] = encoder.fit_transform(df[col])

# Check the result
df.head()

"""One-Hot Encoding"""

# One-hot encode categorical columns
df_encoded = pd.get_dummies(df, drop_first=True)

# Display the encoded DataFrame
df_encoded.head()

"""Feature Scaling"""

# Assuming 'Accident_Severity' is the target, or a similar named column is the target
target_column = 'Accident_Severity'  # Replace with actual column name if different

# Check if the target column exists in the DataFrame
if target_column in df.columns:
    X = df.drop(columns=[target_column])
    y = df[target_column]
else:
    print(f"Error: Column '{target_column}' not found in the DataFrame.")
    # Handle the error, e.g., by raising an exception or skipping the operation
    # ...

"""Train-Test Split"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler # Importing the scaler

# Assuming 'Accident_Severity' is the target, or a similar named column is the target
target_column = 'Accident_Severity'  # Replace with actual column name if different

# Check if the target column exists in the original DataFrame (df) before one-hot encoding
# because df is likely modified in previous cells and no longer has the original target column
if target_column in df.columns:
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Scale the features using StandardScaler
    scaler = StandardScaler() # Creating a scaler object
    X_scaled = scaler.fit_transform(X) # Scaling the features

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
else:
    print(f"Error: Column '{target_column}' not found in the DataFrame.")
    # Handle the error, e.g., by raising an exception or skipping the operation
    # ...

""" Model Building"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# Assuming 'Accident_Severity' is the target, or a similar named column is the target
target_column = 'Accident_Severity'  # Replace with actual column name if different

# Check if the target column exists in the DataFrame
if target_column in df.columns:
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Scale the

"""Evaluation"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


# Assuming 'Accident_Severity' is the target, or a similar

""" Make Predictions from New Input"""

def make_prediction(input_data):
    # Input data should be an array matching the feature structure of the model
    input_scaled = scaler.transform([input_data])
    prediction = model.predict(input_scaled)
    return prediction

"""Convert to DataFrame and Encode"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


# Assuming 'Accident_Severity' is the target, or a similar named column is the target
target_column = 'Accident_Severity'  # Replace with actual column name if different

# Check if the target column exists in the DataFrame
if target_column in df.columns:
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Scale the

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create and fit the StandardScaler
    scaler = StandardScaler()

""" Predict the Final Grade"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


# Assuming 'Accident_Severity' is the target, or a similar named column is the target
target_column = 'Accident_Severity'  # Replace with actual column name if different

# Check if the target column exists in the DataFrame
if target_column in df.columns:
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create and fit the StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train) # Fit on training data and transform

    # Fit the model
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    # Transform the test data using the fitted scaler
    X_test = scaler.transform(X_test)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")
    print(classification_report(y_test, y_pred))
else:
    print(f"Error: Column '{target_column}' not found in the DataFrame.")
    # Handle the error, e.g., by raising an exception or skipping the operation
    # ...

def make_prediction(input_data):
    # Input data should be an array matching the feature structure of the model
    input_scaled = scaler.transform([input_data])
    prediction = model.predict(input_scaled)
    return prediction

"""Deployment - Building an Interactive App"""

!pip install gradioimport gradio as gr

def predict_severity(feature_1, feature_2, feature_3):
    input_data = [feature_1, feature_2, feature_3]
    prediction = make_prediction(input_data)
    return prediction

"""Create the Gradio Interface"""

!pip install gradio
import gradio as gr # This line was missing and had typo


def predict_severity(feature_1, feature_2, feature_3):
    input_data = [feature_1, feature_2, feature_3]
    prediction = make_prediction(input_data)
    return prediction

gr.Interface(fn=predict_severity, inputs=["number", "number", "number"], outputs="text").launch()

"""Student Performance Predictor"""

gr.Interface(fn=predict_severity, inputs=["number", "number", "number"], outputs="text").launch()

